{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_generation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd-SJBkWK6hZ",
        "outputId": "3d2950b0-04cf-40f6-bb2f-c8dfa83b7e9d"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9RrkcCcF5l6"
      },
      "source": [
        "install the requirements file and the old version gpu tensorflow.\r\n",
        "\r\n",
        "requirements file will be in the github folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hjumpn-Cyn78",
        "outputId": "f1fee1ee-05f2-48af-8786-971c130ac35c"
      },
      "source": [
        "!pip install -r '/gdrive/MyDrive/CS470/teamai/requirements.txt'\r\n",
        "!pip install tensorflow-gpu==1.13.1"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n",
            "\u001b[K     |████████████████████████████████| 92.5MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from -r /gdrive/MyDrive/CS470/teamai/requirements.txt (line 5)) (7.0.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from -r /gdrive/MyDrive/CS470/teamai/requirements.txt (line 6)) (1.4.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r /gdrive/MyDrive/CS470/teamai/requirements.txt (line 4)) (0.3.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r /gdrive/MyDrive/CS470/teamai/requirements.txt (line 4)) (1.33.2)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 43.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r /gdrive/MyDrive/CS470/teamai/requirements.txt (line 4)) (3.12.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r /gdrive/MyDrive/CS470/teamai/requirements.txt (line 4)) (1.1.2)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r /gdrive/MyDrive/CS470/teamai/requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r /gdrive/MyDrive/CS470/teamai/requirements.txt (line 4)) (0.35.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r /gdrive/MyDrive/CS470/teamai/requirements.txt (line 4)) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r /gdrive/MyDrive/CS470/teamai/requirements.txt (line 4)) (0.10.0)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 48.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r /gdrive/MyDrive/CS470/teamai/requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r /gdrive/MyDrive/CS470/teamai/requirements.txt (line 4)) (1.18.5)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/74/d72daf8dff5b6566db857cfd088907bb0355f5dd2914c4b3ef065c790735/mock-4.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1->-r /gdrive/MyDrive/CS470/teamai/requirements.txt (line 4)) (50.3.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1->-r /gdrive/MyDrive/CS470/teamai/requirements.txt (line 4)) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1->-r /gdrive/MyDrive/CS470/teamai/requirements.txt (line 4)) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1->-r /gdrive/MyDrive/CS470/teamai/requirements.txt (line 4)) (3.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1->-r /gdrive/MyDrive/CS470/teamai/requirements.txt (line 4)) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1->-r /gdrive/MyDrive/CS470/teamai/requirements.txt (line 4)) (3.4.0)\n",
            "Installing collected packages: mock, tensorflow-estimator, keras-applications, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.2 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n",
            "Collecting tensorflow-gpu==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/b1/0ad4ae02e17ddd62109cd54c291e311c4b5fd09b4d0678d3d6ce4159b0f0/tensorflow_gpu-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (345.2MB)\n",
            "\u001b[K     |████████████████████████████████| 345.2MB 53kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.35.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (3.12.4)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.18.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.33.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.3.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.13.1) (50.3.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu==1.13.1) (4.0.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.4.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kbc9Z9pPKIbk"
      },
      "source": [
        "I used 40 different TTF fonts from naver fonts.\r\n",
        "\r\n",
        "\r\n",
        "Set the directory path into the fonts path, label path, output directory and run the hangul-image-generator file.\r\n",
        "\r\n",
        "\r\n",
        "This will generate irregular image files from the original fonts which can be similar to the handwritings.\r\n",
        "\r\n",
        "Also creates labels in a csv file at the output folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wixLGzcXzc0b",
        "outputId": "8e965003-ac16-48ad-d2fe-b8b835498ce6"
      },
      "source": [
        "!python '/gdrive/My Drive/CS470/teamai/tools/hangul-image-generator.py' --label-file '/gdrive/My Drive/CS470/teamai/labels/2350-common-hangul.txt' --font-dir '/gdrive/My Drive/CS470/teamai/fonts' --output-dir '/gdrive/My Drive/CS470/teamai/image_data_2350'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5120 images generated...\n",
            "10240 images generated...\n",
            "15360 images generated...\n",
            "20480 images generated...\n",
            "25600 images generated...\n",
            "30720 images generated...\n",
            "35840 images generated...\n",
            "40960 images generated...\n",
            "46080 images generated...\n",
            "51200 images generated...\n",
            "56320 images generated...\n",
            "61440 images generated...\n",
            "66560 images generated...\n",
            "71680 images generated...\n",
            "76800 images generated...\n",
            "81920 images generated...\n",
            "87040 images generated...\n",
            "92160 images generated...\n",
            "97280 images generated...\n",
            "102400 images generated...\n",
            "107520 images generated...\n",
            "112640 images generated...\n",
            "117760 images generated...\n",
            "122880 images generated...\n",
            "128000 images generated...\n",
            "133120 images generated...\n",
            "138240 images generated...\n",
            "143360 images generated...\n",
            "148480 images generated...\n",
            "153600 images generated...\n",
            "158720 images generated...\n",
            "163840 images generated...\n",
            "168960 images generated...\n",
            "174080 images generated...\n",
            "179200 images generated...\n",
            "184320 images generated...\n",
            "189440 images generated...\n",
            "194560 images generated...\n",
            "199680 images generated...\n",
            "204800 images generated...\n",
            "209920 images generated...\n",
            "215040 images generated...\n",
            "220160 images generated...\n",
            "225280 images generated...\n",
            "230400 images generated...\n",
            "235520 images generated...\n",
            "240640 images generated...\n",
            "245760 images generated...\n",
            "250880 images generated...\n",
            "256000 images generated...\n",
            "261120 images generated...\n",
            "266240 images generated...\n",
            "271360 images generated...\n",
            "276480 images generated...\n",
            "281600 images generated...\n",
            "286720 images generated...\n",
            "291840 images generated...\n",
            "296960 images generated...\n",
            "302080 images generated...\n",
            "307200 images generated...\n",
            "312320 images generated...\n",
            "317440 images generated...\n",
            "322560 images generated...\n",
            "327680 images generated...\n",
            "332800 images generated...\n",
            "337920 images generated...\n",
            "343040 images generated...\n",
            "348160 images generated...\n",
            "353280 images generated...\n",
            "358400 images generated...\n",
            "363520 images generated...\n",
            "368640 images generated...\n",
            "373760 images generated...\n",
            "Finished generating 376000 images.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTvm2jWgFgqE"
      },
      "source": [
        "From the labeled images, generate the training dataset and the test dataset.\r\n",
        "\r\n",
        "Set the label file, csv file(in the previous output), and the output dir and run the convert-to-tfrecords file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKXMmLqi1kJ6",
        "outputId": "76f4825b-bb01-47ca-d68f-b0a77852fd4a"
      },
      "source": [
        "!python '/gdrive/My Drive/CS470/teamai/tools/convert-to-tfrecords.py' --label-file '/gdrive/My Drive/CS470/teamai/labels/2350-common-hangul.txt' --image-label-csv  '/gdrive/My Drive/CS470/teamai/image_data_2350/labels-map.csv' --output-dir '/gdrive/My Drive/CS470/teamai/tfrecords-output_2350' "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "Processing training set TFRecords...\n",
            "Processed 1000 images...\n",
            "Processed 2000 images...\n",
            "Processed 3000 images...\n",
            "Processed 4000 images...\n",
            "Processed 5000 images...\n",
            "Processed 6000 images...\n",
            "Processed 7000 images...\n",
            "Processed 8000 images...\n",
            "Processed 9000 images...\n",
            "Processed 10000 images...\n",
            "Processed 11000 images...\n",
            "Processed 12000 images...\n",
            "Processed 13000 images...\n",
            "Processed 14000 images...\n",
            "Processed 15000 images...\n",
            "Processed 16000 images...\n",
            "Processed 17000 images...\n",
            "Processed 18000 images...\n",
            "Processed 19000 images...\n",
            "Processed 20000 images...\n",
            "Processed 21000 images...\n",
            "Processed 22000 images...\n",
            "Processed 23000 images...\n",
            "Processed 24000 images...\n",
            "Processed 25000 images...\n",
            "Processed 26000 images...\n",
            "Processed 27000 images...\n",
            "Processed 28000 images...\n",
            "Processed 29000 images...\n",
            "Processed 30000 images...\n",
            "Processed 31000 images...\n",
            "Processed 32000 images...\n",
            "Processed 33000 images...\n",
            "Processed 34000 images...\n",
            "Processed 35000 images...\n",
            "Processed 36000 images...\n",
            "Processed 37000 images...\n",
            "Processed 38000 images...\n",
            "Processed 39000 images...\n",
            "Processed 40000 images...\n",
            "Processed 41000 images...\n",
            "Processed 42000 images...\n",
            "Processed 43000 images...\n",
            "Processed 44000 images...\n",
            "Processed 45000 images...\n",
            "Processed 46000 images...\n",
            "Processed 47000 images...\n",
            "Processed 48000 images...\n",
            "Processed 49000 images...\n",
            "Processed 50000 images...\n",
            "Processed 51000 images...\n",
            "Processed 52000 images...\n",
            "Processed 53000 images...\n",
            "Processed 54000 images...\n",
            "Processed 55000 images...\n",
            "Processed 56000 images...\n",
            "Processed 57000 images...\n",
            "Processed 58000 images...\n",
            "Processed 59000 images...\n",
            "Processed 60000 images...\n",
            "Processed 61000 images...\n",
            "Processed 62000 images...\n",
            "Processed 63000 images...\n",
            "Processed 64000 images...\n",
            "Processed 65000 images...\n",
            "Processed 66000 images...\n",
            "Processed 67000 images...\n",
            "Processed 68000 images...\n",
            "Processed 69000 images...\n",
            "Processed 70000 images...\n",
            "Processed 71000 images...\n",
            "Processed 72000 images...\n",
            "Processed 73000 images...\n",
            "Processed 74000 images...\n",
            "Processed 75000 images...\n",
            "Processed 76000 images...\n",
            "Processed 77000 images...\n",
            "Processed 78000 images...\n",
            "Processed 79000 images...\n",
            "Processed 80000 images...\n",
            "Processed 81000 images...\n",
            "Processed 82000 images...\n",
            "Processed 83000 images...\n",
            "Processed 84000 images...\n",
            "Processed 85000 images...\n",
            "Processed 86000 images...\n",
            "Processed 87000 images...\n",
            "Processed 88000 images...\n",
            "Processed 89000 images...\n",
            "Processed 90000 images...\n",
            "Processed 91000 images...\n",
            "Processed 92000 images...\n",
            "Processed 93000 images...\n",
            "Processed 94000 images...\n",
            "Processed 95000 images...\n",
            "Processed 96000 images...\n",
            "Processed 97000 images...\n",
            "Processed 98000 images...\n",
            "Processed 99000 images...\n",
            "Processed 100000 images...\n",
            "Processed 101000 images...\n",
            "Processed 102000 images...\n",
            "Processed 103000 images...\n",
            "Processed 104000 images...\n",
            "Processed 105000 images...\n",
            "Processed 106000 images...\n",
            "Processed 107000 images...\n",
            "Processed 108000 images...\n",
            "Processed 109000 images...\n",
            "Processed 110000 images...\n",
            "Processed 111000 images...\n",
            "Processed 112000 images...\n",
            "Processed 113000 images...\n",
            "Processed 114000 images...\n",
            "Processed 115000 images...\n",
            "Processed 116000 images...\n",
            "Processed 117000 images...\n",
            "Processed 118000 images...\n",
            "Processed 119000 images...\n",
            "Processed 120000 images...\n",
            "Processed 121000 images...\n",
            "Processed 122000 images...\n",
            "Processed 123000 images...\n",
            "Processed 124000 images...\n",
            "Processed 125000 images...\n",
            "Processed 126000 images...\n",
            "Processed 127000 images...\n",
            "Processed 128000 images...\n",
            "Processed 129000 images...\n",
            "Processed 130000 images...\n",
            "Processed 131000 images...\n",
            "Processed 132000 images...\n",
            "Processed 133000 images...\n",
            "Processed 134000 images...\n",
            "Processed 135000 images...\n",
            "Processed 136000 images...\n",
            "Processed 137000 images...\n",
            "Processed 138000 images...\n",
            "Processed 139000 images...\n",
            "Processed 140000 images...\n",
            "Processed 141000 images...\n",
            "Processed 142000 images...\n",
            "Processed 143000 images...\n",
            "Processed 144000 images...\n",
            "Processed 145000 images...\n",
            "Processed 146000 images...\n",
            "Processed 147000 images...\n",
            "Processed 148000 images...\n",
            "Processed 149000 images...\n",
            "Processed 150000 images...\n",
            "Processed 151000 images...\n",
            "Processed 152000 images...\n",
            "Processed 153000 images...\n",
            "Processed 154000 images...\n",
            "Processed 155000 images...\n",
            "Processed 156000 images...\n",
            "Processed 157000 images...\n",
            "Processed 158000 images...\n",
            "Processed 159000 images...\n",
            "Processed 160000 images...\n",
            "Processed 161000 images...\n",
            "Processed 162000 images...\n",
            "Processed 163000 images...\n",
            "Processed 164000 images...\n",
            "Processed 165000 images...\n",
            "Processed 166000 images...\n",
            "Processed 167000 images...\n",
            "Processed 168000 images...\n",
            "Processed 169000 images...\n",
            "Processed 170000 images...\n",
            "Processed 171000 images...\n",
            "Processed 172000 images...\n",
            "Processed 173000 images...\n",
            "Processed 174000 images...\n",
            "Processed 175000 images...\n",
            "Processed 176000 images...\n",
            "Processed 177000 images...\n",
            "Processed 178000 images...\n",
            "Processed 179000 images...\n",
            "Processed 180000 images...\n",
            "Processed 181000 images...\n",
            "Processed 182000 images...\n",
            "Processed 183000 images...\n",
            "Processed 184000 images...\n",
            "Processed 185000 images...\n",
            "Processed 186000 images...\n",
            "Processed 187000 images...\n",
            "Processed 188000 images...\n",
            "Processed 189000 images...\n",
            "Processed 190000 images...\n",
            "Processed 191000 images...\n",
            "Processed 192000 images...\n",
            "Processed 193000 images...\n",
            "Processed 194000 images...\n",
            "Processed 195000 images...\n",
            "Processed 196000 images...\n",
            "Processed 197000 images...\n",
            "Processed 198000 images...\n",
            "Processed 199000 images...\n",
            "Processed 200000 images...\n",
            "Processed 201000 images...\n",
            "Processed 202000 images...\n",
            "Processed 203000 images...\n",
            "Processed 204000 images...\n",
            "Processed 205000 images...\n",
            "Processed 206000 images...\n",
            "Processed 207000 images...\n",
            "Processed 208000 images...\n",
            "Processed 209000 images...\n",
            "Processed 210000 images...\n",
            "Processed 211000 images...\n",
            "Processed 212000 images...\n",
            "Processed 213000 images...\n",
            "Processed 214000 images...\n",
            "Processed 215000 images...\n",
            "Processed 216000 images...\n",
            "Processed 217000 images...\n",
            "Processed 218000 images...\n",
            "Processed 219000 images...\n",
            "Processed 220000 images...\n",
            "Processed 221000 images...\n",
            "Processed 222000 images...\n",
            "Processed 223000 images...\n",
            "Processed 224000 images...\n",
            "Processed 225000 images...\n",
            "Processed 226000 images...\n",
            "Processed 227000 images...\n",
            "Processed 228000 images...\n",
            "Processed 229000 images...\n",
            "Processed 230000 images...\n",
            "Processed 231000 images...\n",
            "Processed 232000 images...\n",
            "Processed 233000 images...\n",
            "Processed 234000 images...\n",
            "Processed 235000 images...\n",
            "Processed 236000 images...\n",
            "Processed 237000 images...\n",
            "Processed 238000 images...\n",
            "Processed 239000 images...\n",
            "Processed 240000 images...\n",
            "Processed 241000 images...\n",
            "Processed 242000 images...\n",
            "Processed 243000 images...\n",
            "Processed 244000 images...\n",
            "Processed 245000 images...\n",
            "Processed 246000 images...\n",
            "Processed 247000 images...\n",
            "Processed 248000 images...\n",
            "Processed 249000 images...\n",
            "Processed 250000 images...\n",
            "Processed 251000 images...\n",
            "Processed 252000 images...\n",
            "Processed 253000 images...\n",
            "Processed 254000 images...\n",
            "Processed 255000 images...\n",
            "Processed 256000 images...\n",
            "Processed 257000 images...\n",
            "Processed 258000 images...\n",
            "Processed 259000 images...\n",
            "Processed 260000 images...\n",
            "Processed 261000 images...\n",
            "Processed 262000 images...\n",
            "Processed 263000 images...\n",
            "Processed 264000 images...\n",
            "Processed 265000 images...\n",
            "Processed 266000 images...\n",
            "Processed 267000 images...\n",
            "Processed 268000 images...\n",
            "Processed 269000 images...\n",
            "Processed 270000 images...\n",
            "Processed 271000 images...\n",
            "Processed 272000 images...\n",
            "Processed 273000 images...\n",
            "Processed 274000 images...\n",
            "Processed 275000 images...\n",
            "Processed 276000 images...\n",
            "Processed 277000 images...\n",
            "Processed 278000 images...\n",
            "Processed 279000 images...\n",
            "Processed 280000 images...\n",
            "Processed 281000 images...\n",
            "Processed 282000 images...\n",
            "Processed 283000 images...\n",
            "Processed 284000 images...\n",
            "Processed 285000 images...\n",
            "Processed 286000 images...\n",
            "Processed 287000 images...\n",
            "Processed 288000 images...\n",
            "Processed 289000 images...\n",
            "Processed 290000 images...\n",
            "Processed 291000 images...\n",
            "Processed 292000 images...\n",
            "Processed 293000 images...\n",
            "Processed 294000 images...\n",
            "Processed 295000 images...\n",
            "Processed 296000 images...\n",
            "Processed 297000 images...\n",
            "Processed 298000 images...\n",
            "Processed 299000 images...\n",
            "Processed 300000 images...\n",
            "Processed 301000 images...\n",
            "Processed 302000 images...\n",
            "Processed 303000 images...\n",
            "Processed 304000 images...\n",
            "Processed 305000 images...\n",
            "Processed 306000 images...\n",
            "Processed 307000 images...\n",
            "Processed 308000 images...\n",
            "Processed 309000 images...\n",
            "Processed 310000 images...\n",
            "Processed 311000 images...\n",
            "Processed 312000 images...\n",
            "Processed 313000 images...\n",
            "Processed 314000 images...\n",
            "Processed 315000 images...\n",
            "Processed 316000 images...\n",
            "Processed 317000 images...\n",
            "Processed 318000 images...\n",
            "Processed 319000 images...\n",
            "Processing testing set TFRecords...\n",
            "Processed 320000 images...\n",
            "Processed 321000 images...\n",
            "Processed 322000 images...\n",
            "Processed 323000 images...\n",
            "Processed 324000 images...\n",
            "Processed 325000 images...\n",
            "Processed 326000 images...\n",
            "Processed 327000 images...\n",
            "Processed 328000 images...\n",
            "Processed 329000 images...\n",
            "Processed 330000 images...\n",
            "Processed 331000 images...\n",
            "Processed 332000 images...\n",
            "Processed 333000 images...\n",
            "Processed 334000 images...\n",
            "Processed 335000 images...\n",
            "Processed 336000 images...\n",
            "Processed 337000 images...\n",
            "Processed 338000 images...\n",
            "Processed 339000 images...\n",
            "Processed 340000 images...\n",
            "Processed 341000 images...\n",
            "Processed 342000 images...\n",
            "Processed 343000 images...\n",
            "Processed 344000 images...\n",
            "Processed 345000 images...\n",
            "Processed 346000 images...\n",
            "Processed 347000 images...\n",
            "Processed 348000 images...\n",
            "Processed 349000 images...\n",
            "Processed 350000 images...\n",
            "Processed 351000 images...\n",
            "Processed 352000 images...\n",
            "Processed 353000 images...\n",
            "Processed 354000 images...\n",
            "Processed 355000 images...\n",
            "Processed 356000 images...\n",
            "Processed 357000 images...\n",
            "Processed 358000 images...\n",
            "Processed 359000 images...\n",
            "Processed 360000 images...\n",
            "Processed 361000 images...\n",
            "Processed 362000 images...\n",
            "Processed 363000 images...\n",
            "Processed 364000 images...\n",
            "Processed 365000 images...\n",
            "Processed 366000 images...\n",
            "Processed 367000 images...\n",
            "Processed 368000 images...\n",
            "Processed 369000 images...\n",
            "Processed 370000 images...\n",
            "Processed 371000 images...\n",
            "Processed 372000 images...\n",
            "Processed 373000 images...\n",
            "Processed 374000 images...\n",
            "Processed 375000 images...\n",
            "Processed 376000 images...\n",
            "\n",
            "Processed 376000 total images...\n",
            "Number of training examples: 319600\n",
            "Number of testing examples: 56400\n",
            "TFRecords files saved to /gdrive/My Drive/CS470/teamai/tfrecords-output_2350\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3bBIaB0FbA4"
      },
      "source": [
        "Trains the model with the training dataset.\r\n",
        "\r\n",
        "set the labelfile, output directory, and the previous output directory and run the hangul_model file\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeVePHSI3G3S",
        "outputId": "3573e2c3-a3e7-4f35-8efa-c705dfef1a2c"
      },
      "source": [
        "!python '/gdrive/My Drive/CS470/teamai/hangul_model.py' --label-file '/gdrive/My Drive/CS470/teamai/labels/2350-common-hangul.txt' --output-dir '/gdrive/My Drive/CS470/teamai/saved-model_2350_15' --tfrecords-dir '/gdrive/My Drive/CS470/teamai/tfrecords-output_2350'"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "Processing data...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "2020-12-09 19:38:28.342663: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2020-12-09 19:38:28.457891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-09 19:38:28.458622: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1eaa680 executing computations on platform CUDA. Devices:\n",
            "2020-12-09 19:38:28.458654: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
            "2020-12-09 19:38:28.460677: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2020-12-09 19:38:28.461141: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1ea9ce0 executing computations on platform Host. Devices:\n",
            "2020-12-09 19:38:28.461175: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2020-12-09 19:38:28.461309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 15.75GiB freeMemory: 15.44GiB\n",
            "2020-12-09 19:38:28.461333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2020-12-09 19:38:28.462299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-12-09 19:38:28.462323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2020-12-09 19:38:28.462333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2020-12-09 19:38:28.462391: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-12-09 19:38:28.462430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15024 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)\n",
            "2020-12-09 19:38:29.906083: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "Step 0, Training Accuracy 0\n",
            "Step 100, Training Accuracy 0\n",
            "Step 200, Training Accuracy 0.01\n",
            "Step 300, Training Accuracy 0\n",
            "Step 400, Training Accuracy 0\n",
            "Step 500, Training Accuracy 0\n",
            "Step 600, Training Accuracy 0\n",
            "Step 700, Training Accuracy 0\n",
            "Step 800, Training Accuracy 0\n",
            "Step 900, Training Accuracy 0\n",
            "Step 1000, Training Accuracy 0\n",
            "Step 1100, Training Accuracy 0\n",
            "Step 1200, Training Accuracy 0\n",
            "Step 1300, Training Accuracy 0\n",
            "Step 1400, Training Accuracy 0\n",
            "Step 1500, Training Accuracy 0\n",
            "Step 1600, Training Accuracy 0\n",
            "Step 1700, Training Accuracy 0\n",
            "Step 1800, Training Accuracy 0\n",
            "Step 1900, Training Accuracy 0\n",
            "Step 2000, Training Accuracy 0\n",
            "Step 2100, Training Accuracy 0\n",
            "Step 2200, Training Accuracy 0\n",
            "Step 2300, Training Accuracy 0\n",
            "Step 2400, Training Accuracy 0\n",
            "Step 2500, Training Accuracy 0\n",
            "Step 2600, Training Accuracy 0\n",
            "Step 2700, Training Accuracy 0\n",
            "Step 2800, Training Accuracy 0.01\n",
            "Step 2900, Training Accuracy 0.01\n",
            "Step 3000, Training Accuracy 0\n",
            "Step 3100, Training Accuracy 0\n",
            "Step 3200, Training Accuracy 0\n",
            "Step 3300, Training Accuracy 0\n",
            "Step 3400, Training Accuracy 0\n",
            "Step 3500, Training Accuracy 0.01\n",
            "Step 3600, Training Accuracy 0.01\n",
            "Step 3700, Training Accuracy 0\n",
            "Step 3800, Training Accuracy 0\n",
            "Step 3900, Training Accuracy 0.01\n",
            "Step 4000, Training Accuracy 0\n",
            "Step 4100, Training Accuracy 0\n",
            "Step 4200, Training Accuracy 0\n",
            "Step 4300, Training Accuracy 0.01\n",
            "Step 4400, Training Accuracy 0\n",
            "Step 4500, Training Accuracy 0.01\n",
            "Step 4600, Training Accuracy 0.01\n",
            "Step 4700, Training Accuracy 0.07\n",
            "Step 4800, Training Accuracy 0.03\n",
            "Step 4900, Training Accuracy 0.01\n",
            "Step 5000, Training Accuracy 0\n",
            "Step 5100, Training Accuracy 0.03\n",
            "Step 5200, Training Accuracy 0.02\n",
            "Step 5300, Training Accuracy 0.01\n",
            "Step 5400, Training Accuracy 0.02\n",
            "Step 5500, Training Accuracy 0.04\n",
            "Step 5600, Training Accuracy 0.03\n",
            "Step 5700, Training Accuracy 0.04\n",
            "Step 5800, Training Accuracy 0.01\n",
            "Step 5900, Training Accuracy 0.04\n",
            "Step 6000, Training Accuracy 0.03\n",
            "Step 6100, Training Accuracy 0.04\n",
            "Step 6200, Training Accuracy 0.07\n",
            "Step 6300, Training Accuracy 0.04\n",
            "Step 6400, Training Accuracy 0.06\n",
            "Step 6500, Training Accuracy 0.08\n",
            "Step 6600, Training Accuracy 0.02\n",
            "Step 6700, Training Accuracy 0.05\n",
            "Step 6800, Training Accuracy 0.1\n",
            "Step 6900, Training Accuracy 0.09\n",
            "Step 7000, Training Accuracy 0.11\n",
            "Step 7100, Training Accuracy 0.07\n",
            "Step 7200, Training Accuracy 0.11\n",
            "Step 7300, Training Accuracy 0.07\n",
            "Step 7400, Training Accuracy 0.06\n",
            "Step 7500, Training Accuracy 0.08\n",
            "Step 7600, Training Accuracy 0.13\n",
            "Step 7700, Training Accuracy 0.12\n",
            "Step 7800, Training Accuracy 0.13\n",
            "Step 7900, Training Accuracy 0.15\n",
            "Step 8000, Training Accuracy 0.15\n",
            "Step 8100, Training Accuracy 0.22\n",
            "Step 8200, Training Accuracy 0.18\n",
            "Step 8300, Training Accuracy 0.18\n",
            "Step 8400, Training Accuracy 0.17\n",
            "Step 8500, Training Accuracy 0.14\n",
            "Step 8600, Training Accuracy 0.19\n",
            "Step 8700, Training Accuracy 0.18\n",
            "Step 8800, Training Accuracy 0.21\n",
            "Step 8900, Training Accuracy 0.22\n",
            "Step 9000, Training Accuracy 0.28\n",
            "Step 9100, Training Accuracy 0.26\n",
            "Step 9200, Training Accuracy 0.17\n",
            "Step 9300, Training Accuracy 0.31\n",
            "Step 9400, Training Accuracy 0.25\n",
            "Step 9500, Training Accuracy 0.23\n",
            "Step 9600, Training Accuracy 0.36\n",
            "Step 9700, Training Accuracy 0.25\n",
            "Step 9800, Training Accuracy 0.34\n",
            "Step 9900, Training Accuracy 0.32\n",
            "Step 10000, Training Accuracy 0.3\n",
            "Step 10100, Training Accuracy 0.31\n",
            "Step 10200, Training Accuracy 0.35\n",
            "Step 10300, Training Accuracy 0.34\n",
            "Step 10400, Training Accuracy 0.35\n",
            "Step 10500, Training Accuracy 0.42\n",
            "Step 10600, Training Accuracy 0.34\n",
            "Step 10700, Training Accuracy 0.36\n",
            "Step 10800, Training Accuracy 0.38\n",
            "Step 10900, Training Accuracy 0.35\n",
            "Step 11000, Training Accuracy 0.43\n",
            "Step 11100, Training Accuracy 0.34\n",
            "Step 11200, Training Accuracy 0.38\n",
            "Step 11300, Training Accuracy 0.39\n",
            "Step 11400, Training Accuracy 0.44\n",
            "Step 11500, Training Accuracy 0.4\n",
            "Step 11600, Training Accuracy 0.45\n",
            "Step 11700, Training Accuracy 0.46\n",
            "Step 11800, Training Accuracy 0.48\n",
            "Step 11900, Training Accuracy 0.47\n",
            "Step 12000, Training Accuracy 0.58\n",
            "Step 12100, Training Accuracy 0.49\n",
            "Step 12200, Training Accuracy 0.42\n",
            "Step 12300, Training Accuracy 0.56\n",
            "Step 12400, Training Accuracy 0.48\n",
            "Step 12500, Training Accuracy 0.58\n",
            "Step 12600, Training Accuracy 0.52\n",
            "Step 12700, Training Accuracy 0.47\n",
            "Step 12800, Training Accuracy 0.59\n",
            "Step 12900, Training Accuracy 0.53\n",
            "Step 13000, Training Accuracy 0.6\n",
            "Step 13100, Training Accuracy 0.69\n",
            "Step 13200, Training Accuracy 0.55\n",
            "Step 13300, Training Accuracy 0.58\n",
            "Step 13400, Training Accuracy 0.56\n",
            "Step 13500, Training Accuracy 0.57\n",
            "Step 13600, Training Accuracy 0.61\n",
            "Step 13700, Training Accuracy 0.54\n",
            "Step 13800, Training Accuracy 0.57\n",
            "Step 13900, Training Accuracy 0.65\n",
            "Step 14000, Training Accuracy 0.65\n",
            "Step 14100, Training Accuracy 0.57\n",
            "Step 14200, Training Accuracy 0.73\n",
            "Step 14300, Training Accuracy 0.71\n",
            "Step 14400, Training Accuracy 0.74\n",
            "Step 14500, Training Accuracy 0.69\n",
            "Step 14600, Training Accuracy 0.76\n",
            "Step 14700, Training Accuracy 0.59\n",
            "Step 14800, Training Accuracy 0.71\n",
            "Step 14900, Training Accuracy 0.7\n",
            "Step 15000, Training Accuracy 0.71\n",
            "Step 15100, Training Accuracy 0.7\n",
            "Step 15200, Training Accuracy 0.68\n",
            "Step 15300, Training Accuracy 0.78\n",
            "Step 15400, Training Accuracy 0.73\n",
            "Step 15500, Training Accuracy 0.66\n",
            "Step 15600, Training Accuracy 0.79\n",
            "Step 15700, Training Accuracy 0.8\n",
            "Step 15800, Training Accuracy 0.75\n",
            "Step 15900, Training Accuracy 0.69\n",
            "Step 16000, Training Accuracy 0.75\n",
            "Step 16100, Training Accuracy 0.82\n",
            "Step 16200, Training Accuracy 0.75\n",
            "Step 16300, Training Accuracy 0.83\n",
            "Step 16400, Training Accuracy 0.75\n",
            "Step 16500, Training Accuracy 0.8\n",
            "Step 16600, Training Accuracy 0.79\n",
            "Step 16700, Training Accuracy 0.79\n",
            "Step 16800, Training Accuracy 0.77\n",
            "Step 16900, Training Accuracy 0.88\n",
            "Step 17000, Training Accuracy 0.75\n",
            "Step 17100, Training Accuracy 0.84\n",
            "Step 17200, Training Accuracy 0.83\n",
            "Step 17300, Training Accuracy 0.8\n",
            "Step 17400, Training Accuracy 0.78\n",
            "Step 17500, Training Accuracy 0.82\n",
            "Step 17600, Training Accuracy 0.85\n",
            "Step 17700, Training Accuracy 0.78\n",
            "Step 17800, Training Accuracy 0.78\n",
            "Step 17900, Training Accuracy 0.83\n",
            "Step 18000, Training Accuracy 0.81\n",
            "Step 18100, Training Accuracy 0.86\n",
            "Step 18200, Training Accuracy 0.73\n",
            "Step 18300, Training Accuracy 0.88\n",
            "Step 18400, Training Accuracy 0.86\n",
            "Step 18500, Training Accuracy 0.84\n",
            "Step 18600, Training Accuracy 0.86\n",
            "Step 18700, Training Accuracy 0.89\n",
            "Step 18800, Training Accuracy 0.88\n",
            "Step 18900, Training Accuracy 0.84\n",
            "Step 19000, Training Accuracy 0.89\n",
            "Step 19100, Training Accuracy 0.86\n",
            "Step 19200, Training Accuracy 0.91\n",
            "Step 19300, Training Accuracy 0.93\n",
            "Step 19400, Training Accuracy 0.85\n",
            "Step 19500, Training Accuracy 0.95\n",
            "Step 19600, Training Accuracy 0.89\n",
            "Step 19700, Training Accuracy 0.89\n",
            "Step 19800, Training Accuracy 0.84\n",
            "Step 19900, Training Accuracy 0.85\n",
            "Step 20000, Training Accuracy 0.94\n",
            "Step 20100, Training Accuracy 0.84\n",
            "Step 20200, Training Accuracy 0.85\n",
            "Step 20300, Training Accuracy 0.98\n",
            "Step 20400, Training Accuracy 0.93\n",
            "Step 20500, Training Accuracy 0.89\n",
            "Step 20600, Training Accuracy 0.91\n",
            "Step 20700, Training Accuracy 0.85\n",
            "Step 20800, Training Accuracy 0.91\n",
            "Step 20900, Training Accuracy 0.92\n",
            "Step 21000, Training Accuracy 0.9\n",
            "Step 21100, Training Accuracy 0.89\n",
            "Step 21200, Training Accuracy 0.97\n",
            "Step 21300, Training Accuracy 0.89\n",
            "Step 21400, Training Accuracy 0.93\n",
            "Step 21500, Training Accuracy 0.94\n",
            "Step 21600, Training Accuracy 0.84\n",
            "Step 21700, Training Accuracy 0.89\n",
            "Step 21800, Training Accuracy 0.94\n",
            "Step 21900, Training Accuracy 0.94\n",
            "Step 22000, Training Accuracy 0.9\n",
            "Step 22100, Training Accuracy 0.93\n",
            "Step 22200, Training Accuracy 0.96\n",
            "Step 22300, Training Accuracy 0.95\n",
            "Step 22400, Training Accuracy 0.88\n",
            "Step 22500, Training Accuracy 0.98\n",
            "Step 22600, Training Accuracy 0.92\n",
            "Step 22700, Training Accuracy 0.91\n",
            "Step 22800, Training Accuracy 0.9\n",
            "Step 22900, Training Accuracy 0.97\n",
            "Step 23000, Training Accuracy 0.95\n",
            "Step 23100, Training Accuracy 0.95\n",
            "Step 23200, Training Accuracy 0.91\n",
            "Step 23300, Training Accuracy 0.94\n",
            "Step 23400, Training Accuracy 0.92\n",
            "Step 23500, Training Accuracy 0.93\n",
            "Step 23600, Training Accuracy 0.97\n",
            "Step 23700, Training Accuracy 0.93\n",
            "Step 23800, Training Accuracy 0.93\n",
            "Step 23900, Training Accuracy 0.96\n",
            "Step 24000, Training Accuracy 0.94\n",
            "Step 24100, Training Accuracy 0.96\n",
            "Step 24200, Training Accuracy 0.96\n",
            "Step 24300, Training Accuracy 0.94\n",
            "Step 24400, Training Accuracy 0.94\n",
            "Step 24500, Training Accuracy 0.94\n",
            "Step 24600, Training Accuracy 0.97\n",
            "Step 24700, Training Accuracy 0.92\n",
            "Step 24800, Training Accuracy 0.95\n",
            "Step 24900, Training Accuracy 0.93\n",
            "Step 25000, Training Accuracy 0.96\n",
            "Step 25100, Training Accuracy 0.95\n",
            "Step 25200, Training Accuracy 0.95\n",
            "Step 25300, Training Accuracy 0.95\n",
            "Step 25400, Training Accuracy 0.96\n",
            "Step 25500, Training Accuracy 0.93\n",
            "Step 25600, Training Accuracy 0.95\n",
            "Step 25700, Training Accuracy 0.96\n",
            "Step 25800, Training Accuracy 0.96\n",
            "Step 25900, Training Accuracy 0.95\n",
            "Step 26000, Training Accuracy 0.96\n",
            "Step 26100, Training Accuracy 0.95\n",
            "Step 26200, Training Accuracy 0.93\n",
            "Step 26300, Training Accuracy 0.97\n",
            "Step 26400, Training Accuracy 0.98\n",
            "Step 26500, Training Accuracy 0.96\n",
            "Step 26600, Training Accuracy 0.96\n",
            "Step 26700, Training Accuracy 0.93\n",
            "Step 26800, Training Accuracy 0.98\n",
            "Step 26900, Training Accuracy 0.94\n",
            "Step 27000, Training Accuracy 0.95\n",
            "Step 27100, Training Accuracy 0.97\n",
            "Step 27200, Training Accuracy 0.96\n",
            "Step 27300, Training Accuracy 0.95\n",
            "Step 27400, Training Accuracy 0.98\n",
            "Step 27500, Training Accuracy 0.96\n",
            "Step 27600, Training Accuracy 0.94\n",
            "Step 27700, Training Accuracy 0.97\n",
            "Step 27800, Training Accuracy 0.99\n",
            "Step 27900, Training Accuracy 0.91\n",
            "Step 28000, Training Accuracy 0.99\n",
            "Step 28100, Training Accuracy 0.99\n",
            "Step 28200, Training Accuracy 0.97\n",
            "Step 28300, Training Accuracy 0.98\n",
            "Step 28400, Training Accuracy 0.98\n",
            "Step 28500, Training Accuracy 1\n",
            "Step 28600, Training Accuracy 0.94\n",
            "Step 28700, Training Accuracy 0.96\n",
            "Step 28800, Training Accuracy 1\n",
            "Step 28900, Training Accuracy 0.99\n",
            "Step 29000, Training Accuracy 0.98\n",
            "Step 29100, Training Accuracy 0.97\n",
            "Step 29200, Training Accuracy 0.97\n",
            "Step 29300, Training Accuracy 0.98\n",
            "Step 29400, Training Accuracy 0.98\n",
            "Step 29500, Training Accuracy 1\n",
            "Step 29600, Training Accuracy 0.98\n",
            "Step 29700, Training Accuracy 0.97\n",
            "Step 29800, Training Accuracy 0.99\n",
            "Step 29900, Training Accuracy 0.98\n",
            "Step 30000, Training Accuracy 0.98\n",
            "Step 30100, Training Accuracy 0.99\n",
            "Step 30200, Training Accuracy 0.98\n",
            "Step 30300, Training Accuracy 0.95\n",
            "Step 30400, Training Accuracy 0.97\n",
            "Step 30500, Training Accuracy 0.98\n",
            "Step 30600, Training Accuracy 0.98\n",
            "Step 30700, Training Accuracy 0.93\n",
            "Step 30800, Training Accuracy 0.98\n",
            "Step 30900, Training Accuracy 0.98\n",
            "Step 31000, Training Accuracy 1\n",
            "Step 31100, Training Accuracy 1\n",
            "Step 31200, Training Accuracy 0.99\n",
            "Step 31300, Training Accuracy 0.96\n",
            "Step 31400, Training Accuracy 0.99\n",
            "Step 31500, Training Accuracy 0.98\n",
            "Step 31600, Training Accuracy 0.97\n",
            "Step 31700, Training Accuracy 0.97\n",
            "Step 31800, Training Accuracy 0.98\n",
            "Step 31900, Training Accuracy 1\n",
            "Step 32000, Training Accuracy 0.98\n",
            "Step 32100, Training Accuracy 0.97\n",
            "Step 32200, Training Accuracy 0.96\n",
            "Step 32300, Training Accuracy 0.99\n",
            "Step 32400, Training Accuracy 0.99\n",
            "Step 32500, Training Accuracy 0.98\n",
            "Step 32600, Training Accuracy 0.98\n",
            "Step 32700, Training Accuracy 0.98\n",
            "Step 32800, Training Accuracy 1\n",
            "Step 32900, Training Accuracy 1\n",
            "Step 33000, Training Accuracy 0.99\n",
            "Step 33100, Training Accuracy 0.98\n",
            "Step 33200, Training Accuracy 0.98\n",
            "Step 33300, Training Accuracy 0.98\n",
            "Step 33400, Training Accuracy 0.99\n",
            "Step 33500, Training Accuracy 0.99\n",
            "Step 33600, Training Accuracy 0.99\n",
            "Step 33700, Training Accuracy 0.98\n",
            "Step 33800, Training Accuracy 1\n",
            "Step 33900, Training Accuracy 1\n",
            "Step 34000, Training Accuracy 0.98\n",
            "Step 34100, Training Accuracy 0.98\n",
            "Step 34200, Training Accuracy 0.98\n",
            "Step 34300, Training Accuracy 1\n",
            "Step 34400, Training Accuracy 0.98\n",
            "Step 34500, Training Accuracy 0.98\n",
            "Step 34600, Training Accuracy 0.97\n",
            "Step 34700, Training Accuracy 0.99\n",
            "Step 34800, Training Accuracy 0.98\n",
            "Step 34900, Training Accuracy 0.98\n",
            "Step 35000, Training Accuracy 0.99\n",
            "Step 35100, Training Accuracy 0.97\n",
            "Step 35200, Training Accuracy 0.99\n",
            "Step 35300, Training Accuracy 1\n",
            "Step 35400, Training Accuracy 0.98\n",
            "Step 35500, Training Accuracy 0.99\n",
            "Step 35600, Training Accuracy 0.97\n",
            "Step 35700, Training Accuracy 0.98\n",
            "Step 35800, Training Accuracy 0.99\n",
            "Step 35900, Training Accuracy 0.98\n",
            "Step 36000, Training Accuracy 0.98\n",
            "Step 36100, Training Accuracy 0.99\n",
            "Step 36200, Training Accuracy 1\n",
            "Step 36300, Training Accuracy 0.99\n",
            "Step 36400, Training Accuracy 0.99\n",
            "Step 36500, Training Accuracy 0.99\n",
            "Step 36600, Training Accuracy 0.99\n",
            "Step 36700, Training Accuracy 0.97\n",
            "Step 36800, Training Accuracy 1\n",
            "Step 36900, Training Accuracy 1\n",
            "Step 37000, Training Accuracy 0.99\n",
            "Step 37100, Training Accuracy 0.98\n",
            "Step 37200, Training Accuracy 1\n",
            "Step 37300, Training Accuracy 0.98\n",
            "Step 37400, Training Accuracy 1\n",
            "Step 37500, Training Accuracy 0.98\n",
            "Step 37600, Training Accuracy 1\n",
            "Step 37700, Training Accuracy 1\n",
            "Step 37800, Training Accuracy 0.98\n",
            "Step 37900, Training Accuracy 0.97\n",
            "Step 38000, Training Accuracy 0.99\n",
            "Step 38100, Training Accuracy 1\n",
            "Step 38200, Training Accuracy 1\n",
            "Step 38300, Training Accuracy 1\n",
            "Step 38400, Training Accuracy 0.99\n",
            "Step 38500, Training Accuracy 1\n",
            "Step 38600, Training Accuracy 1\n",
            "Step 38700, Training Accuracy 0.98\n",
            "Step 38800, Training Accuracy 1\n",
            "Step 38900, Training Accuracy 0.99\n",
            "Step 39000, Training Accuracy 0.99\n",
            "Step 39100, Training Accuracy 0.98\n",
            "Step 39200, Training Accuracy 1\n",
            "Step 39300, Training Accuracy 1\n",
            "Step 39400, Training Accuracy 1\n",
            "Step 39500, Training Accuracy 0.99\n",
            "Step 39600, Training Accuracy 1\n",
            "Step 39700, Training Accuracy 1\n",
            "Step 39800, Training Accuracy 0.99\n",
            "Step 39900, Training Accuracy 0.99\n",
            "Step 40000, Training Accuracy 1\n",
            "Step 40100, Training Accuracy 0.99\n",
            "Step 40200, Training Accuracy 1\n",
            "Step 40300, Training Accuracy 0.98\n",
            "Step 40400, Training Accuracy 0.99\n",
            "Step 40500, Training Accuracy 0.99\n",
            "Step 40600, Training Accuracy 0.99\n",
            "Step 40700, Training Accuracy 1\n",
            "Step 40800, Training Accuracy 1\n",
            "Step 40900, Training Accuracy 1\n",
            "Step 41000, Training Accuracy 1\n",
            "Step 41100, Training Accuracy 1\n",
            "Step 41200, Training Accuracy 1\n",
            "Step 41300, Training Accuracy 0.99\n",
            "Step 41400, Training Accuracy 0.99\n",
            "Step 41500, Training Accuracy 1\n",
            "Step 41600, Training Accuracy 0.99\n",
            "Step 41700, Training Accuracy 0.99\n",
            "Step 41800, Training Accuracy 0.98\n",
            "Step 41900, Training Accuracy 1\n",
            "Step 42000, Training Accuracy 0.99\n",
            "Step 42100, Training Accuracy 0.98\n",
            "Step 42200, Training Accuracy 1\n",
            "Step 42300, Training Accuracy 1\n",
            "Step 42400, Training Accuracy 1\n",
            "Step 42500, Training Accuracy 0.99\n",
            "Step 42600, Training Accuracy 0.99\n",
            "Step 42700, Training Accuracy 0.99\n",
            "Step 42800, Training Accuracy 0.98\n",
            "Step 42900, Training Accuracy 0.99\n",
            "Step 43000, Training Accuracy 1\n",
            "Step 43100, Training Accuracy 0.99\n",
            "Step 43200, Training Accuracy 0.99\n",
            "Step 43300, Training Accuracy 1\n",
            "Step 43400, Training Accuracy 1\n",
            "Step 43500, Training Accuracy 0.98\n",
            "Step 43600, Training Accuracy 1\n",
            "Step 43700, Training Accuracy 0.99\n",
            "Step 43800, Training Accuracy 1\n",
            "Step 43900, Training Accuracy 1\n",
            "Step 44000, Training Accuracy 1\n",
            "Step 44100, Training Accuracy 1\n",
            "Step 44200, Training Accuracy 0.99\n",
            "Step 44300, Training Accuracy 1\n",
            "Step 44400, Training Accuracy 0.98\n",
            "Step 44500, Training Accuracy 0.99\n",
            "Step 44600, Training Accuracy 0.99\n",
            "Step 44700, Training Accuracy 0.99\n",
            "Step 44800, Training Accuracy 0.99\n",
            "Step 44900, Training Accuracy 1\n",
            "Step 45000, Training Accuracy 1\n",
            "Step 45100, Training Accuracy 1\n",
            "Step 45200, Training Accuracy 1\n",
            "Step 45300, Training Accuracy 0.99\n",
            "Step 45400, Training Accuracy 0.98\n",
            "Step 45500, Training Accuracy 1\n",
            "Step 45600, Training Accuracy 0.99\n",
            "Step 45700, Training Accuracy 1\n",
            "Step 45800, Training Accuracy 1\n",
            "Step 45900, Training Accuracy 1\n",
            "Step 46000, Training Accuracy 1\n",
            "Step 46100, Training Accuracy 0.99\n",
            "Step 46200, Training Accuracy 1\n",
            "Step 46300, Training Accuracy 1\n",
            "Step 46400, Training Accuracy 0.99\n",
            "Step 46500, Training Accuracy 1\n",
            "Step 46600, Training Accuracy 0.98\n",
            "Step 46700, Training Accuracy 0.99\n",
            "Step 46800, Training Accuracy 1\n",
            "Step 46900, Training Accuracy 0.98\n",
            "Step 47000, Training Accuracy 0.99\n",
            "Step 47100, Training Accuracy 0.99\n",
            "Step 47200, Training Accuracy 1\n",
            "Step 47300, Training Accuracy 0.99\n",
            "Step 47400, Training Accuracy 0.99\n",
            "Step 47500, Training Accuracy 1\n",
            "Step 47600, Training Accuracy 0.99\n",
            "Step 47700, Training Accuracy 1\n",
            "Step 47800, Training Accuracy 1\n",
            "Step 47900, Training Accuracy 1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "Testing model...\n",
            "Testing Accuracy 0.9890780141843971\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/freeze_graph.py:249: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.gfile.GFile.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "2020-12-09 19:52:02.648558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2020-12-09 19:52:02.648689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-12-09 19:52:02.648708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2020-12-09 19:52:02.648723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2020-12-09 19:52:02.648835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15024 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/freeze_graph.py:232: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.compat.v1.graph_util.extract_sub_graph\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/optimize_for_inference_lib.py:113: remove_training_nodes (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.compat.v1.graph_util.remove_training_nodes\n",
            "Inference optimized graph saved at: /gdrive/My Drive/CS470/teamai/saved-model_2350_15/optimized_hangul_tensorflow.pb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSBEnWdFFNFF"
      },
      "source": [
        "This is for testing, put the file path in the first arguement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOSSzvihynv7"
      },
      "source": [
        "!python '/gdrive/My Drive/CS470/teamai/tools/classify-hangul.py' '/content/new1.jpeg' --label-file '/gdrive/My Drive/CS470/teamai/labels/2350-common-hangul.txt'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}